{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-23T19:25:30.907177Z","iopub.status.busy":"2024-10-23T19:25:30.906754Z","iopub.status.idle":"2024-10-23T19:25:35.623965Z","shell.execute_reply":"2024-10-23T19:25:35.623026Z","shell.execute_reply.started":"2024-10-23T19:25:30.907132Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c877ed9e869741e591b5bf94a3029d91","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"067db1cee5ea4fbfb3fc61b9a8f56ef1","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e95d47b88054ff083485faba3c4ec84","version_major":2,"version_minor":0},"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aae2f11d07e94c8f92706c88257fc0ba","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bae732af80694a2daa92e17331b98c45","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","ds = load_dataset(\"rajpurkar/squad\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:25:37.698286Z","iopub.status.busy":"2024-10-23T19:25:37.697334Z","iopub.status.idle":"2024-10-23T19:25:39.837395Z","shell.execute_reply":"2024-10-23T19:25:39.836590Z","shell.execute_reply.started":"2024-10-23T19:25:37.698245Z"},"trusted":true},"outputs":[],"source":["train_dataset=[]\n","train_dataset_length=ds[\"train\"].num_rows\n","for index in range(10000):\n","    train_dataset.append({\"context\":ds[\"train\"][index][\"context\"],\"question\":ds[\"train\"][index][\"question\"]})"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:25:42.974301Z","iopub.status.busy":"2024-10-23T19:25:42.973894Z","iopub.status.idle":"2024-10-23T19:25:46.575191Z","shell.execute_reply":"2024-10-23T19:25:46.574385Z","shell.execute_reply.started":"2024-10-23T19:25:42.974268Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f392f0529cc497e8c83006e67df9ed3","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","class QADataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        context = self.data[idx]['context']\n","        question = self.data[idx]['question']\n","        encoding_context = self.tokenizer(\n","            context,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","        \n","        encoding_question = self.tokenizer(\n","            question,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","        \n","        input_ids = encoding_context['input_ids'].flatten()\n","        labels =encoding_question['input_ids'].flatten()\n","        attention_mask = encoding_context['attention_mask'].flatten()  \n","\n","        return {\n","            'input_ids': input_ids,\n","            'labels': labels,\n","            'attention_mask': attention_mask \n","        }\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:25:50.967561Z","iopub.status.busy":"2024-10-23T19:25:50.966679Z","iopub.status.idle":"2024-10-23T19:25:57.824969Z","shell.execute_reply":"2024-10-23T19:25:57.824137Z","shell.execute_reply.started":"2024-10-23T19:25:50.967518Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6c52e2db84d4069bdd470d2f652d2c9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5b3a89e7c6e49b180228d2127fd2bef","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca50f65020424fc59056816c6c803c3a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebbfaa00e06041ad9d193ad9db90c178","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8cd80497f1d4c5aa16e321f79b64a10","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#!pip install bitnet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#from bitnet import replace_linears_in_hf \n","#replace_linears_in_hf(model)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:26:09.781030Z","iopub.status.busy":"2024-10-23T19:26:09.780420Z","iopub.status.idle":"2024-10-23T19:26:09.786085Z","shell.execute_reply":"2024-10-23T19:26:09.785029Z","shell.execute_reply.started":"2024-10-23T19:26:09.780989Z"},"trusted":true},"outputs":[],"source":["qg_dataset = QADataset(train_dataset, tokenizer)\n","train_loader = DataLoader(qg_dataset, batch_size=4, shuffle=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:26:12.925728Z","iopub.status.busy":"2024-10-23T19:26:12.925366Z","iopub.status.idle":"2024-10-23T19:52:03.361793Z","shell.execute_reply":"2024-10-23T19:52:03.360833Z","shell.execute_reply.started":"2024-10-23T19:26:12.925693Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1, Loss: 0.059901099652051926\n"]},{"data":{"text/plain":["('./fine_tuned_t5_qg/tokenizer_config.json',\n"," './fine_tuned_t5_qg/special_tokens_map.json',\n"," './fine_tuned_t5_qg/spiece.model',\n"," './fine_tuned_t5_qg/added_tokens.json',\n"," './fine_tuned_t5_qg/tokenizer.json')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import AdamW\n","model.train()\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","epochs = 1\n","for epoch in range(epochs):\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        attention_mask = batch['attention_mask'].to(device) \n","        \n","        optimizer.zero_grad()\n","        outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n","\n","\n","model.save_pretrained(\"./fine_tuned_t5_qg\")\n","tokenizer.save_pretrained(\"./fine_tuned_t5_qg\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:53:48.255921Z","iopub.status.busy":"2024-10-23T19:53:48.255248Z","iopub.status.idle":"2024-10-23T19:53:50.396140Z","shell.execute_reply":"2024-10-23T19:53:50.395224Z","shell.execute_reply.started":"2024-10-23T19:53:48.255876Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["question: What percentage of the incoming class admitted to Notre Dame in fall 2015?\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/fine_tuned_t5_qg\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/fine_tuned_t5_qg\")\n","model.eval()\n","def generate_question(context):\n","    inputs = tokenizer.encode(context, return_tensors='pt')\n","    attention_mask = (inputs != tokenizer.pad_token_id).long() \n","    outputs = model.generate(inputs, attention_mask=attention_mask, max_length=100, num_return_sequences=1)\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","\n","\n","context = \"\"\"\n","Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%). The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities. The university practices a non-restrictive early action policy that allows admitted students to consider admission to Notre Dame as well as any other colleges to which they were accepted. 1,400 of the 3,577 (39.1%) were admitted under the early action plan. Admitted students came from 1,311 high schools and the average student traveled more than 750 miles to Notre Dame, making it arguably the most representative university in the United States. While all entering students begin in the College of the First Year of Studies, 25% have indicated they plan to study in the liberal arts or social sciences, 24% in engineering, 24% in business, 24% in science, and 3% in architecture.\n","\"\"\"\n","question = generate_question(context)\n","print(f\"question: {question}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-23T19:55:10.293533Z","iopub.status.busy":"2024-10-23T19:55:10.292576Z","iopub.status.idle":"2024-10-23T19:56:08.271113Z","shell.execute_reply":"2024-10-23T19:56:08.270103Z","shell.execute_reply.started":"2024-10-23T19:55:10.293491Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["QGCheckpoint.zip created successfully!\n"]}],"source":["import zipfile\n","import os\n","folder_to_compress = '/kaggle/working/fine_tuned_t5_qg'  \n","zip_file_name = 'QGCheckpoint.zip' \n","with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n","    for root, dirs, files in os.walk(folder_to_compress):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            zip_file.write(file_path, os.path.relpath(file_path, folder_to_compress))\n","\n","print(f'{zip_file_name} created successfully!')\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
